# Автодополнение текста для мобильных приложений

Система автодополнения текста для мобильных приложений, сравнивающая кастомную LSTM-модель с базовой моделью DistilGPT2.

---

## Описание

Проект реализует и сравнивает два подхода к генерации текста для автодополнения на мобильных устройствах:
- **LSTM** — легковесная модель последовательностей, обученная с нуля и оптимизированная для инференса на устройстве
- **DistilGPT2** — дистиллированный трансформер-базелайн, предобученный на большом английском корпусе

Несмотря на более крупный корпус предобучения DistilGPT2, кастомная LSTM выдаёт конкурентоспособные и в некоторых случаях более контекстно уместные дополнения, при этом будучи значительно меньше и быстрее, что делает её предпочтительным выбором для развёртывания на мобильных устройствах.

### Ключевые особенности:
- Кастомная LSTM-модель, обученная с нуля на целевом корпусе (разговорные тексты Twitter)
- DistilGPT2 для сравнения
- Оценка на основе ROUGE (ROUGE-1, ROUGE-2, ROUGE-L)
- Отслеживание perplexity и кросс-энтропийного loss
- Примеры генерации с качественным анализом
- Мобильно-ориентированный дизайн: легковесная модель, быстрый инференс

---

## Структура проекта

```
text_autocompletion/
├── data/
│   ├── raw/                          # Сырые текстовые данные Twitter
│   └── processed/                    # Токенизированные и разделённые данные
│           ├── train/                # Обучающая выборка
│           ├── val/                  # Валидационная выборка
│           └── test/                 # Тестовая выборка
├── src/
│   ├── get_raw_data.py               # Скачивание сырых данных
│   ├── preprocess_data.py            # Очистка и предобработка текста
│   ├── tokenize_split_data.py        # Токенизация и разделение train/val/test
│   ├── lstm_model.py                 # Архитектура LSTM модели
│   ├── lstm_train.py                 # Цикл обучения LSTM
│   ├── eval_lstm.py                  # Оценка LSTM (loss, ROUGE)
│   └── eval_transformer_pipeline.py  # Оценка DistilGPT2
├── models/
│   ├── lstm_best.pt                  # Лучший чекпоинт LSTM
│   ├── gpt2_generation_examples.json # Примеры генерации DistilGPT2
│   └── test_generation_examples.json # Примеры генерации LSTM на тесте
├── results/                          # Графики, метрики, анализ
├── requirements.txt
└── README.md
```

---

## Установка

### Требования
- Python 3.8+
- PyTorch
- transformers (HuggingFace)
- rouge-score
- numpy, pandas

### Железо
- Обучение проводилось на NVIDIA GeForce RTX 3060 (27M параметров)
- Оценка также тестировалась на Google Colab T4

### Установка зависимостей

```bash
# Windows
.venv\Scripts\activate.bat

# Linux/Mac
source .venv/bin/activate

# Установка пакетов
pip install -r requirements.txt
```

---

## Использование

### 1. Скачивание сырых данных

```bash
python -m src.get_raw_data
```

### 2. Предобработка данных

Очистка и нормализация сырых текстов Twitter.

```bash
python -m src.preprocess_data
```

### 3. Токенизация и разделение

Токенизация корпуса и разделение на train/val/test.

```bash
python -m src.tokenize_split_data
```

### 4. Определение LSTM модели

Определяет архитектуру LSTM с embedding-слоем и проекционным выходом.

```bash
python -m src.lstm_model
```

### 5. Обучение LSTM

Обучает LSTM на обучающей выборке с чекпоинтами на валидации.

```bash
python -m src.lstm_train
```

### 6. Оценка LSTM

Вычисляет loss, perplexity и ROUGE-метрики на тестовой выборке. Генерирует примеры дополнения текста.

```bash
python -m src.eval_lstm
```

### 7. Оценка DistilGPT2

Запускает тот же пайплайн оценки на DistilGPT2 для прямого сравнения.

```bash
python -m src.eval_transformer_pipeline
```

---

## Архитектура модели

### LSTM (Основная модель)

```
Входные токены → Embedding-слой → LSTM-слои → Проекционный выход → Предсказание следующего токена
```

- **Параметры:** 27 миллионов
- Легковесная: не требует предобучения, обучается полностью на целевом корпусе
- Оптимизирована для быстрого инференса на мобильных устройствах
- Конкурентоспособное качество с DistilGPT2 на этой задаче

### DistilGPT2 (Базовая модель)

- Предобучена на большом английском корпусе
- Используется как точка отсчёта для оценки производительности LSTM
- Тяжелее и медленнее LSTM — не подходит для развёртывания на устройстве без дополнительной дистилляции или квантизации

---

## Результаты

### Сравнение метрик

| Метрика | LSTM (Test) | DistilGPT2 (Val) |
|---------|-------------|------------------|
| **Loss** | **4.4529** | 7.4887 |
| **Perplexity** | **85.88** | 1787.65 |
| **ROUGE-1** | **0.0728 (±0.1084)** | 0.0645 |
| **ROUGE-2** | **0.0063 (±0.0438)** | 0.0060 |
| **ROUGE-L** | **0.0678 (±0.1028)** | 0.0591 |

- LSTM достигает **более низкого perplexity (85.88 vs 1787.65)** и **более низкого loss (4.45 vs 7.49)**, что указывает на лучшую адаптацию к целевому домену
- ROUGE-метрики схожи у обеих моделей (~0.06-0.07), подтверждая сопоставимую способность к текстовому совпадению
- Несмотря на похожий ROUGE, дополнения LSTM часто более контекстно уместны

### Примеры генерации (LSTM на тестовой выборке)

| Префикс | Ground Truth | Выход LSTM | ROUGE-1 |
|---------|--------------|------------|---------|
| "ok so somehow ashwee managed to make a whole bottle of purple punky explode..." | "maybe it's a sign i need sleep? but i dun wanna." | "good night, it was a great day! my birthday. going to sleep" | 0.250 |
| "i know, i know... but i can't get my" | "hair wet!!!" | "phone on my twitter." | 0.000 |
| "easy week then 98 minutes to go..." | "...... i just wanted to give yall smiles." | ". it's so nice to see how you can be a kid" | 0.105 |
| "i am jealous. all the shows in my area were far away..." | "see this show have a blast!" | "see the movie on bday party" | 0.167 |

### Примеры генерации (DistilGPT2 на валидации)

| Префикс | Ground Truth | Выход DistilGPT2 |
|---------|--------------|------------------|
| "brill poor rabbit. maybe we should get a frog as well" | "to eat the flies?" | ", then we can make a frog ourselves." |
| "ah yes, space is so limited here! must work with what i have!" | "yours is looking great, too!" | "if i have to try it, i will not" |
| "cool. did i see something about a free lunch..." | "rewards the weather is gorgeous" | ". I can't remember if i noticed a sign" |

Полные примеры генерации сохранены в:
- `models/test_generation_examples.json` (LSTM)
- `models/gpt2_generation_examples.json` (DistilGPT2)

---

## Анализ

### Почему ROUGE-метрики низкие

Низкий ROUGE-1 у обеих моделей (~0.06-0.07) отражает фундаментальную сложность задачи: предсказание точного продолжения **разговорного текста Twitter** является сложным даже для людей. Твиты часто представляют собой поток сознания, нелогичны и сильно зависят от контекста. Выходы контекстно правдоподобны, но редко совпадают слово в слово с ground truth — что ожидаемо для открытой генерации на неформальном тексте.

### LSTM vs DistilGPT2

| Аспект | LSTM | DistilGPT2 |
|--------|------|------------|
| **Loss** | 4.45 (лучше) | 7.49 |
| **Perplexity** | 85.88 (лучше) | 1787.65 |
| **ROUGE-1** | 0.0728 (немного лучше) | 0.0645 |
| **Контекстное качество** | Часто более уместно | Иногда обобщённо |
| **Размер модели** | Маленький | Большой |
| **Скорость** | Быстро | Медленно |
| **Предобучение** | Нет (обучена только на целевом домене) | Большой корпус |

- **LSTM** превосходит DistilGPT2 по loss и perplexity, что указывает на лучшую адаптацию к целевому домену (неформальный текст Twitter
Salida

# Автодополнение текста для мобильных приложений

Система автодополнения текста для мобильных приложений, сравнивающая кастомную LSTM-модель с базовой моделью DistilGPT2.

---

## Описание

Проект реализует и сравнивает два подхода к генерации текста для автодополнения на мобильных устройствах:
- **LSTM** — легковесная модель последовательностей, обученная с нуля и оптимизированная для инференса на устройстве
- **DistilGPT2** — дистиллированный трансформер-базелайн, предобученный на большом английском корпусе

Несмотря на более крупный корпус предобучения DistilGPT2, кастомная LSTM выдаёт конкурентоспособные и в некоторых случаях более контекстно уместные дополнения, при этом будучи значительно меньше и быстрее — что делает её предпочтительным выбором для развёртывания на мобильных устройствах.

### Ключевые особенности:
- Кастомная LSTM-модель, обученная с нуля на целевом корпусе (разговорные тексты Twitter)
- DistilGPT2 для сравнения
- Оценка на основе ROUGE (ROUGE-1, ROUGE-2, ROUGE-L)
- Отслеживание perplexity и кросс-энтропийного loss
- Примеры генерации с качественным анализом
- Мобильно-ориентированный дизайн: легковесная модель, быстрый инференс

---

## Структура проекта

```
text_autocompletion/
├── data/
│   ├── raw/                          # Сырые текстовые данные Twitter
│   └── processed/                    # Токенизированные и разделённые данные
│           ├── train/                # Обучающая выборка
│           ├── val/                  # Валидационная выборка
│           └── test/                 # Тестовая выборка
├── src/
│   ├── get_raw_data.py               # Скачивание сырых данных
│   ├── preprocess_data.py            # Очистка и предобработка текста
│   ├── tokenize_split_data.py        # Токенизация и разделение train/val/test
│   ├── lstm_model.py                 # Архитектура LSTM модели
│   ├── lstm_train.py                 # Цикл обучения LSTM
│   ├── eval_lstm.py                  # Оценка LSTM (loss, ROUGE)
│   └── eval_transformer_pipeline.py  # Оценка DistilGPT2
├── models/
│   ├── lstm_best.pt                  # Лучший чекпоинт LSTM
│   ├── gpt2_generation_examples.json # Примеры генерации DistilGPT2
│   └── test_generation_examples.json # Примеры генерации LSTM на тесте
├── results/                          # Графики, метрики, анализ
├── requirements.txt
└── README.md
```

---

## Установка

### Требования
- Python 3.8+
- PyTorch
- transformers (HuggingFace)
- rouge-score
- numpy, pandas

### Железо
- Обучение проводилось на NVIDIA GeForce RTX 3060 (27M параметров)
- Оценка также тестировалась на Google Colab T4

### Установка зависимостей

```bash
# Windows
.venv\Scripts\activate.bat

# Linux/Mac
source .venv/bin/activate

# Установка пакетов
pip install -r requirements.txt
```

---

## Использование

### 1. Скачивание сырых данных

```bash
python -m src.get_raw_data
```

### 2. Предобработка данных

Очистка и нормализация сырых текстов Twitter.

```bash
python -m src.preprocess_data
```

### 3. Токенизация и разделение

Токенизация корпуса и разделение на train/val/test.

```bash
python -m src.tokenize_split_data
```

### 4. Определение LSTM модели

Определяет архитектуру LSTM с embedding-слоем и проекционным выходом.

```bash
python -m src.lstm_model
```

### 5. Обучение LSTM

Обучает LSTM на обучающей выборке с чекпоинтами на валидации.

```bash
python -m src.lstm_train
```

### 6. Оценка LSTM

Вычисляет loss, perplexity и ROUGE-метрики на тестовой выборке. Генерирует примеры дополнения текста.

```bash
python -m src.eval_lstm
```

### 7. Оценка DistilGPT2

Запускает тот же пайплайн оценки на DistilGPT2 для прямого сравнения.

```bash
python -m src.eval_transformer_pipeline
```

---

## Архитектура модели

### LSTM (Основная модель)

```
Входные токены → Embedding-слой → LSTM-слои → Проекционный выход → Предсказание следующего токена
```

- **Параметры:** 27 миллионов
- Легковесная: не требует предобучения, обучается полностью на целевом корпусе
- Оптимизирована для быстрого инференса на мобильных устройствах
- Конкурентоспособное качество с DistilGPT2 на этой задаче

### DistilGPT2 (Базовая модель)

- Предобучена на большом английском корпусе
- Используется как точка отсчёта для оценки производительности LSTM
- Тяжелее и медленнее LSTM — не подходит для развёртывания на устройстве без дополнительной дистилляции или квантизации

---

## Результаты

### Сравнение метрик

| Метрика | LSTM (Test) | DistilGPT2 (Val) |
|---------|-------------|------------------|
| **Loss** | **4.4529** | 7.4887 |
| **Perplexity** | **85.88** | 1787.65 |
| **ROUGE-1** | **0.0728 (±0.1084)** | 0.0645 |
| **ROUGE-2** | **0.0063 (±0.0438)** | 0.0060 |
| **ROUGE-L** | **0.0678 (±0.1028)** | 0.0591 |

- LSTM достигает **более низкого perplexity (85.88 vs 1787.65)** и **более низкого loss (4.45 vs 7.49)**, что указывает на лучшую адаптацию к целевому домену
- ROUGE-метрики схожи у обеих моделей (~0.06-0.07), подтверждая сопоставимую способность к текстовому совпадению
- Несмотря на похожий ROUGE, дополнения LSTM часто более контекстно уместны

### Примеры генерации (LSTM на тестовой выборке)

| Префикс | Ground Truth | Выход LSTM | ROUGE-1 |
|---------|--------------|------------|---------|
| "ok so somehow ashwee managed to make a whole bottle of purple punky explode..." | "maybe it's a sign i need sleep? but i dun wanna." | "good night, it was a great day! my birthday. going to sleep" | 0.250 |
| "i know, i know... but i can't get my" | "hair wet!!!" | "phone on my twitter." | 0.000 |
| "easy week then 98 minutes to go..." | "...... i just wanted to give yall smiles." | ". it's so nice to see how you can be a kid" | 0.105 |
| "i am jealous. all the shows in my area were far away..." | "see this show have a blast!" | "see the movie on bday party" | 0.167 |

### Примеры генерации (DistilGPT2 на валидации)

| Префикс | Ground Truth | Выход DistilGPT2 |
|---------|--------------|------------------|
| "brill poor rabbit. maybe we should get a frog as well" | "to eat the flies?" | ", then we can make a frog ourselves." |
| "ah yes, space is so limited here! must work with what i have!" | "yours is looking great, too!" | "if i have to try it, i will not" |
| "cool. did i see something about a free lunch..." | "rewards the weather is gorgeous" | ". I can't remember if i noticed a sign" |

Полные примеры генерации сохранены в:
- `models/test_generation_examples.json` (LSTM)
- `models/gpt2_generation_examples.json` (DistilGPT2)

---

## Анализ

### Почему ROUGE-метрики низкие

Низкий ROUGE-1 у обеих моделей (~0.06-0.07) отражает фундаментальную сложность задачи: предсказание точного продолжения **разговорного текста Twitter** является сложным даже для людей. Твиты часто представляют собой поток сознания, нелогичны и сильно зависят от контекста. Выходы контекстно правдоподобны, но редко совпадают слово в слово с ground truth — что ожидаемо для открытой генерации на неформальном тексте.

### LSTM vs DistilGPT2

| Аспект | LSTM | DistilGPT2 |
|--------|------|------------|
| **Loss** | 4.45 (лучше) | 7.49 |
| **Perplexity** | 85.88 (лучше) | 1787.65 |
| **ROUGE-1** | 0.0728 (немного лучше) | 0.0645 |
| **Контекстное качество** | Часто более уместно | Иногда обобщённо |
| **Размер модели** | Маленький | Большой |
| **Скорость** | Быстро | Медленно |
| **Предобучение** | Нет (обучена только на целевом домене) | Большой корпус |

- **LSTM** превосходит DistilGPT2 по loss и perplexity, что указывает на лучшую адаптацию к целевому домену (неформальный текст Twitter)
- **DistilGPT2** получает пользу от предобучения, но не дообучена на этом конкретном корпусе, что приводит к более высокому perplexity
- **Для мобильного развёртывания** LSTM — явный победитель: меньший размер, быстрый инференс, лучшее соответствие домену

---

## Выводы

- LSTM выдаёт относительно осмысленные текстовые дополнения для автодополнения в Twitter, хотя идеальное совпадение объективно сложно на неформальных текстах, представляющих поток сознания
- DistilGPT2, благодаря предобучению на большом корпусе, выдаёт сопоставимые результаты, но с более высоким loss и perplexity на этом конкретном домене
- Метрики показывают похожий ROUGE (~0.06-0.07 у обеих), но LSTM достигает значительно лучшего perplexity (85.88 vs 1787.65)
- Для мобильных приложений LSTM предпочтительнее из-за меньшего размера и более быстрого инференса, особенно при дальнейшем дообучении на более связных текстах и большем корпусе
- Обучение на RTX 3060 (27M параметров) было значительно быстрее, чем на Colab T4, как для тренировки, так и для оценки

---

## Возможные улучшения

- **Больший и более связный обучающий корпус** — обучение LSTM на формальных разговорных текстах (например, чаты поддержки клиентов, SMS) улучшит связность и снизит perplexity
- **Дообучение DistilGPT2** — адаптация трансформера к целевому домену (Twitter) может закрыть разрыв в perplexity
- **Квантизация** — применение INT8 или динамической квантизации к любой из моделей дополнительно сократит время инференса на мобильных устройствах
- **Beam search** — замена жадного декодирования на beam search может произвести более плавные и разнообразные дополнения
- **Персонализация под пользователя** — настройка модели на историю набора текста конкретного пользователя для более релевантных предложений
- **Адаптация к домену** — обучение на нескольких текстовых доменах (SMS, email, социальные сети) для лучшей генерализации в разных случаях использования

---

## Технологии

- **PyTorch** — фреймворк для обучения и реализация LSTM
- **Transformers** — модель DistilGPT2 (HuggingFace)
- **rouge-score** — вычисление ROUGE-метрик
- **numpy / pandas** — обработка данных
- **matplotlib** — кривые обучения и визуализация
